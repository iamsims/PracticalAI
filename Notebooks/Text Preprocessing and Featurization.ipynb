{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db =sqlite3.connect('../Data/database.sqlite')\n",
    "data= pd.read_sql_query(\"\"\" SELECT * from Reviews WHERE Score!=3 LIMIT 5000\"\"\",db) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.303862e+09</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.346976e+09</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.219018e+09</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.307923e+09</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.350778e+09</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id   ProductId          UserId                      ProfileName  \\\n",
       "0  1.0  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1  2.0  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2  3.0  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3  4.0  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4  5.0  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score          Time  \\\n",
       "0                   1.0                     1.0    5.0  1.303862e+09   \n",
       "1                   0.0                     0.0    1.0  1.346976e+09   \n",
       "2                   1.0                     1.0    4.0  1.219018e+09   \n",
       "3                   3.0                     3.0    2.0  1.307923e+09   \n",
       "4                   0.0                     0.0    5.0  1.350778e+09   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_score=score.map(lambda x:int(x>3))\n",
    "data.loc['Score'] = classified_score\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R115TNMSPFT9I7</td>\n",
       "      <td>Breyton</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R11DNU2NBKQ23Z</td>\n",
       "      <td>Kim Cieszykowski</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>Penguin Chick</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#oc-R12KPBODL2B5ZD</td>\n",
       "      <td>Christopher P. Presta</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId             ProfileName  COUNT(*)\n",
       "0  #oc-R115TNMSPFT9I7                 Breyton         2\n",
       "1  #oc-R11D9D7SHXIJB9  Louis E. Emory \"hoppy\"         3\n",
       "2  #oc-R11DNU2NBKQ23Z        Kim Cieszykowski         2\n",
       "3  #oc-R11O5J5ZVQE25C           Penguin Chick         3\n",
       "4  #oc-R12KPBODL2B5ZD   Christopher P. Presta         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all the users who have given reviews more than 1 time'''\n",
    "display0 = pd.read_sql_query(\"\"\"                   \n",
    "SELECT UserId, ProfileName, COUNT(*)\n",
    "FROM Reviews\n",
    "GROUP BY UserId\n",
    "HAVING COUNT(*)>1\n",
    "\"\"\", db)\n",
    "display0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R103C0QSV1DF5E</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R109MU5OBBZ59U</td>\n",
       "      <td>AayGee</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R10LFEMQEW6QGZ</td>\n",
       "      <td>Julie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R10LT57ZGIB140</td>\n",
       "      <td>dipr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#oc-R10UA029WVWIUI</td>\n",
       "      <td>Kim D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId ProfileName  COUNT(*)\n",
       "0  #oc-R103C0QSV1DF5E           C         1\n",
       "1  #oc-R109MU5OBBZ59U      AayGee         1\n",
       "2  #oc-R10LFEMQEW6QGZ       Julie         1\n",
       "3  #oc-R10LT57ZGIB140        dipr         1\n",
       "4  #oc-R10UA029WVWIUI       Kim D         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all the unique users '''\n",
    "display1 = pd.read_sql_query(\"\"\"                   \n",
    "SELECT UserId, ProfileName, COUNT(*)\n",
    "FROM Reviews\n",
    "GROUP BY UserId\n",
    "\"\"\", db)\n",
    "display1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80668 users who gave more than 1 reviews.\n",
      "256059 unique users out of 568454 reviews.\n"
     ]
    }
   ],
   "source": [
    "print(str(display0.shape[0]) + \" users who gave more than 1 reviews.\" )\n",
    "print(str(display1.shape[0])+ \" unique users out of \" +  str(display1['COUNT(*)'].sum()) + \" reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>Penguin Chick</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R12MGTQS5KZZRV</td>\n",
       "      <td>SKY2110 \"SKY2110\"</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R13NNUL4EKL4FL</td>\n",
       "      <td>N. Chernyavskaya</td>\n",
       "      <td>1348358400</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#oc-R14ZSRYW2YB41B</td>\n",
       "      <td>A. Crafton</td>\n",
       "      <td>1346284800</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId             ProfileName        Time   ProductId  \\\n",
       "0  #oc-R11D9D7SHXIJB9  Louis E. Emory \"hoppy\"  1342396800  B005HG9ESG   \n",
       "1  #oc-R11O5J5ZVQE25C           Penguin Chick  1346889600  B005HG9ESG   \n",
       "2  #oc-R12MGTQS5KZZRV       SKY2110 \"SKY2110\"  1344211200  B005HG9ESG   \n",
       "3  #oc-R13NNUL4EKL4FL        N. Chernyavskaya  1348358400  B005HG9ESG   \n",
       "4  #oc-R14ZSRYW2YB41B              A. Crafton  1346284800  B005HG9ESG   \n",
       "\n",
       "   COUNT(*)  \n",
       "0         3  \n",
       "1         3  \n",
       "2         3  \n",
       "3         3  \n",
       "4         3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''duplicate reviews '''\n",
    "\n",
    "display = pd.read_sql_query(\"\"\" \n",
    "SELECT UserId, ProfileName,Time,ProductId, COUNT(*)\n",
    "FROM Reviews\n",
    "GROUP BY UserId, Time \n",
    "HAVING COUNT(*)>2\n",
    "\"\"\", db)\n",
    "display.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#oc-R11D9D7SHXIJB9'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.iloc[0]['UserId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>B005HG9ESG</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>B005HG9ERW</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId             ProfileName        Time   ProductId  \\\n",
       "0  #oc-R11D9D7SHXIJB9  Louis E. Emory \"hoppy\"  1342396800  B005HG9ESG   \n",
       "1  #oc-R11D9D7SHXIJB9  Louis E. Emory \"hoppy\"  1342396800  B005HG9ERW   \n",
       "2  #oc-R11D9D7SHXIJB9  Louis E. Emory \"hoppy\"  1342396800  B005HG9ET0   \n",
       "\n",
       "                                                Text  \n",
       "0  My wife has recurring extreme muscle spasms, u...  \n",
       "1  My wife has recurring extreme muscle spasms, u...  \n",
       "2  My wife has recurring extreme muscle spasms, u...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = pd.read_sql_query(\"\"\" \n",
    "SELECT UserId, ProfileName,Time,ProductId,Text\n",
    "FROM Reviews\n",
    "WHERE UserID = \"#oc-R11D9D7SHXIJB9\"\n",
    "\"\"\", db)\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the same user reviews different product Id in same time which is not possible.  While examining the productIds B005HG9ESG, B005HG9ERW, B005HG9ET0 in Amazon, we found out that they referred to same product-Essentia Water.Hence, this is a data error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data=data.sort_values('ProductId', axis=0, ascending=True, inplace=False,ignore_index=True, kind='quicksort',na_position='last')\n",
    "sorted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.72005598880223"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final['Id'].size*1.0)/(data['Id'].size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Text</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UserId              ProfileName        Time   ProductId  \\\n",
       "0  A2V0I904FH7ABY                      Ram  1212883200  B001EQ55RW   \n",
       "1  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"  1224892800  B000MIDROQ   \n",
       "\n",
       "                                                Text  HelpfulnessNumerator  \\\n",
       "0  It was almost a 'love at first bite' - the per...                     3   \n",
       "1  My son loves spaghetti so I didn't hesitate or...                     3   \n",
       "\n",
       "   HelpfulnessDenominator  \n",
       "0                       2  \n",
       "1                       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distorted_data = pd.read_sql_query(\"\"\" \n",
    "SELECT UserId, ProfileName,Time,ProductId,Text,HelpfulnessNumerator, HelpfulnessDenominator\n",
    "FROM Reviews\n",
    "WHERE HelpfulnessNumerator> HelpfulnessDenominator\n",
    "\"\"\", db)\n",
    "distorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is this $[...] when the same product is available for $[...] here?<br />http://www.amazon.com/VICTOR-FLY-MAGNET-BAIT-REFILL/dp/B00004RBDY<br /><br />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.\n",
      "I bought this same brand from an online Indian grocery store that usually has excellent products.  I was able to turn it into cream/butter, using my super blender and adding some water, but it barely had any flavor.  I usually buy the Chao Kah brand of coconut cream (which is quite tasty and flavorful) and read another review for a different product for making your own coconut cream.  My complaint is not the shreds or texture.  Mine was just virtually tasteless.\n",
      "I use these to keep my finicky toddler's protein levels up. What's great, is that they're delicious just about any way you can imagine cooking a sausage!<br /><br />A favorite Recipe: Sautee one large onion, about a tablespoon each dried basil and oregano, and some salt and pepper in about 2T of butter. Add sausage, that's been cut into coins, and russet potatoes cut into 1/4\" thick rounds. Cover all with chicken stock and simmer until potatoes are soft but not dissolved. Throw in a few handfuls of pre-washed baby spinach (I usually use about a half a pound, and frozen would work fine, just put it in a minute earlier). Stir until the spinach is cooked, and then serve! Enjoy!\n"
     ]
    }
   ],
   "source": [
    "sample_0 = final['Text'].values[0]\n",
    "print(sample_0)\n",
    "\n",
    "sample_1= final['Text'].values[10]\n",
    "print(sample_1)\n",
    "\n",
    "sample_2 = final['Text'].values[11]\n",
    "print(sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_0 = re.sub(r\"http\\S+\", \"\", sample_0) #https://stackoverflow.com/a/40823105/4084039\n",
    "sample_1 = re.sub(r\"http\\S+\", \"\", sample_1) # remove urls from text python: \n",
    "sample_2 = re.sub(r\"http\\S+\", \"\", sample_2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why is this $[...] when the same product is available for $[...] here?<br /> /><br />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.',\n",
       " 'I bought this same brand from an online Indian grocery store that usually has excellent products.  I was able to turn it into cream/butter, using my super blender and adding some water, but it barely had any flavor.  I usually buy the Chao Kah brand of coconut cream (which is quite tasty and flavorful) and read another review for a different product for making your own coconut cream.  My complaint is not the shreds or texture.  Mine was just virtually tasteless.',\n",
       " 'I use these to keep my finicky toddler\\'s protein levels up. What\\'s great, is that they\\'re delicious just about any way you can imagine cooking a sausage!<br /><br />A favorite Recipe: Sautee one large onion, about a tablespoon each dried basil and oregano, and some salt and pepper in about 2T of butter. Add sausage, that\\'s been cut into coins, and russet potatoes cut into 1/4\" thick rounds. Cover all with chicken stock and simmer until potatoes are soft but not dissolved. Throw in a few handfuls of pre-washed baby spinach (I usually use about a half a pound, and frozen would work fine, just put it in a minute earlier). Stir until the spinach is cooked, and then serve! Enjoy!')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_0, sample_1, sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why is this $[...] when the same product is available for $[...] here? />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.',\n",
       " 'I bought this same brand from an online Indian grocery store that usually has excellent products.  I was able to turn it into cream/butter, using my super blender and adding some water, but it barely had any flavor.  I usually buy the Chao Kah brand of coconut cream (which is quite tasty and flavorful) and read another review for a different product for making your own coconut cream.  My complaint is not the shreds or texture.  Mine was just virtually tasteless.',\n",
       " 'I use these to keep my finicky toddler\\'s protein levels up. What\\'s great, is that they\\'re delicious just about any way you can imagine cooking a sausage!A favorite Recipe: Sautee one large onion, about a tablespoon each dried basil and oregano, and some salt and pepper in about 2T of butter. Add sausage, that\\'s been cut into coins, and russet potatoes cut into 1/4\" thick rounds. Cover all with chicken stock and simmer until potatoes are soft but not dissolved. Throw in a few handfuls of pre-washed baby spinach (I usually use about a half a pound, and frozen would work fine, just put it in a minute earlier). Stir until the spinach is cooked, and then serve! Enjoy!')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "soup0 = BeautifulSoup(sample_0, 'lxml')\n",
    "\n",
    "\n",
    "soup1 = BeautifulSoup(sample_1, 'lxml')\n",
    "\n",
    "\n",
    "soup2 = BeautifulSoup(sample_2, 'lxml')\n",
    "soup0.text, soup1.text, soup2.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I use these to keep my finicky toddler is protein levels up. What is great, is that they are delicious just about any way you can imagine cooking a sausage!A favorite Recipe: Sautee one large onion, about a tablespoon each dried basil and oregano, and some salt and pepper in about 2T of butter. Add sausage, that is been cut into coins, and russet potatoes cut into 1/4\" thick rounds. Cover all with chicken stock and simmer until potatoes are soft but not dissolved. Throw in a few handfuls of pre-washed baby spinach (I usually use about a half a pound, and frozen would work fine, just put it in a minute earlier). Stir until the spinach is cooked, and then serve! Enjoy!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "sent_2 = decontracted(soup2.text)\n",
    "print(sent_2)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is this $[...] when the same product is available for $[...] here? />The Victor  and  traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.\n"
     ]
    }
   ],
   "source": [
    "sent_0 = re.sub(\"\\S*\\d\\S*\", \"\", soup0.text).strip()\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is this when the same product is available for here The Victor and traps are unreal of course total fly genocide Pretty stinky but only right nearby \n"
     ]
    }
   ],
   "source": [
    "sent_0 = re.sub('[^A-Za-z0-9]+', ' ', sent_0)\n",
    "print(sent_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sims/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "\n",
    "\n",
    "stopwords= set(['the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [00:02<00:00, 1893.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "\n",
    "for sentance in tqdm(final['Text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('thank goodness mexgrocer love pico pica sauce cannot find product local grocery stores anymore internet shopping makes living podunk town bearable no long lists saved occasional trip big city first name basis ups fedex drivers usps shipments hit miss fit mailbox great not saddle head town pico pica favorite product good love',\n",
       " 'although seems like great product definitely not minced cans say minced not ground beef cat likes minced not chunks not ground although eat ground cat food sometimes inviting delicious yet walks away hate packaging lies contents product shipping speedy value great long deliver promise stuck food cat wont eat',\n",
       " 'wow far two two star reviews one obviously no idea ordering wants crispy cookies hey sorry reviews nobody good beyond reminding us look ordering chocolate oatmeal cookies not like combination not order type cookie find combo quite nice really oatmeal sort calms rich chocolate flavor gives cookie sort coconut type consistency let also remember tastes differ given opinion soft chewy cookies advertised not crispy cookies blurb would say crispy rather chewy happen like raw cookie dough however not see taste like raw cookie dough soft however confusion yes stick together soft cookies tend not individually wrapped would add cost oh yeah chocolate chip cookies tend somewhat sweet want something hard crisp suggest nabiso ginger snaps want cookie soft chewy tastes like combination chocolate oatmeal give try place second order')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviews[15],preprocessed_reviews[100],preprocessed_reviews[1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "- most primitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aahhhs', 'aback', 'abandon', 'abates', 'abbott', 'abby', 'abdominal', 'abiding', 'ability']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "count_vect.fit(preprocessed_reviews)\n",
    "print( count_vect.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique words in the corpus =  12997\n"
     ]
    }
   ],
   "source": [
    "print('No. of unique words in the corpus = ',len(count_vect.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape of the reviews  (4986, 12997)\n"
     ]
    }
   ],
   "source": [
    "final_counts = count_vect.transform(preprocessed_reviews)\n",
    "print('Transformed shape of the reviews ',final_counts.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'able find', 'able get', 'absolute', 'absolutely', 'absolutely delicious', 'absolutely love', 'absolutely no', 'according']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngram_range=(1,2) makes unigram and bigrams\n",
    "#min_df=10, it ignore all the words whose document frequency is less than 10\n",
    "#max_features=5000, build a vocabulary that only consider the top 5000 features based on document frequency\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "count_vect.fit(preprocessed_reviews)\n",
    "print(count_vect.get_feature_names()[:10])\n",
    "len(count_vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape of the reviews  (4986, 3136)\n",
      "No. of unique words including both unigrams and bigrams  3136\n"
     ]
    }
   ],
   "source": [
    "final_ngram_counts = count_vect.transform(preprocessed_reviews)\n",
    "\n",
    "print('Transformed shape of the reviews ',final_ngram_counts.get_shape())\n",
    "print(\"No. of unique words including both unigrams and bigrams \", final_ngram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some unique words in the corpus ['ability', 'able', 'able find', 'able get', 'absolute', 'absolutely', 'absolutely delicious', 'absolutely love', 'absolutely no', 'according']\n",
      "No. of unique words  3136\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_vect.fit(preprocessed_reviews)\n",
    "print(\"some unique words in the corpus\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print(\"No. of unique words \",len(tf_idf_vect.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- best for featurization of words \n",
    "- deep learning based technique, state of art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sentence=[]\n",
    "for sentence in preprocessed_reviews:\n",
    "    list_of_sentence.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('excellent', 0.9944062829017639), ('alternative', 0.9941544532775879), ('regular', 0.9932729601860046), ('select', 0.992916464805603), ('healthy', 0.9928731322288513), ('snack', 0.9928361773490906), ('want', 0.9927979111671448), ('think', 0.992700457572937), ('amazing', 0.9926773309707642), ('character', 0.9926738739013672)]\n",
      "[('normal', 0.9995217323303223), ('person', 0.999500572681427), ('everything', 0.9994964599609375), ('type', 0.9994933009147644), ('simply', 0.9994809627532959), ('wife', 0.9994708895683289), ('together', 0.9994684457778931), ('style', 0.9994637966156006), ('perhaps', 0.9994599223136902), ('impressed', 0.9994575381278992)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#training from the data corpse\n",
    "w2v_model=Word2Vec(list_of_sentence,min_count=5,size=50, workers=4)\n",
    "print(w2v_model.wv.most_similar('great'))\n",
    "print(w2v_model.wv.most_similar('worst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3817,\n",
       " ['product',\n",
       "  'available',\n",
       "  'course',\n",
       "  'total',\n",
       "  'pretty',\n",
       "  'stinky',\n",
       "  'right',\n",
       "  'nearby',\n",
       "  'used',\n",
       "  'ca'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "len(w2v_words),w2v_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31276128, -0.06566499,  0.1608323 , -0.34536588, -0.01395172,\n",
       "       -0.39674166,  0.03380366, -0.0236141 , -0.29837874,  0.11624591,\n",
       "       -0.44152173, -0.05751957,  0.10805692,  0.00942176, -0.13781674,\n",
       "       -0.03384238,  0.5850935 ,  0.18682131,  0.19209814,  0.02883594,\n",
       "        0.03699137, -0.5988023 , -0.6265162 ,  0.05789305,  0.03849778,\n",
       "        0.05900403, -0.02137214, -0.30381128, -0.25098008, -0.01349276,\n",
       "        0.19758217,  0.22091766,  0.089909  ,  0.1263907 , -0.23011   ,\n",
       "       -0.01024266,  0.16831417,  0.2576866 ,  0.18156648, -0.02201283,\n",
       "        0.11036674, -0.05499217, -0.10109375, -0.05277149,  0.22030534,\n",
       "        0.17509386,  0.0787142 , -0.0810428 ,  0.11067777, -0.2380039 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['play']  #gives word embedding if the word is present in the vocabulary\n",
    "                      # doesn't gove result for the  word ='hero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_to_use_google_w2v=True \n",
    "is_your_ram_gt_16g=False #make it true to load google's w2v model \n",
    "#download and extract GoogleNews-vectors-negative300.bin from:\n",
    "#https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "if want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    w2v_model=KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  3817\n",
      "sample words  ['product', 'available', 'course', 'total', 'pretty', 'stinky', 'right', 'nearby', 'used', 'ca', 'not', 'beat', 'great', 'received', 'shipment', 'could', 'hardly', 'wait', 'try', 'love', 'call', 'instead', 'removed', 'easily', 'daughter', 'designed', 'printed', 'use', 'car', 'windows', 'beautifully', 'shop', 'program', 'going', 'lot', 'fun', 'everywhere', 'like', 'tv', 'computer', 'really', 'good', 'idea', 'final', 'outstanding', 'window', 'everybody', 'asks', 'bought', 'made']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting  text into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [00:07<00:00, 695.55it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences =[]\n",
    "for sentence in  tqdm(list_of_sentence):\n",
    "    emb_vec = np.zeros(50) #the words are 50 dim-vectors\n",
    "    count=0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words:\n",
    "            vec= w2v_model.wv[word]\n",
    "            emb_vec += vec \n",
    "            count+=1\n",
    "    if (count!=0): emb_vec/=count\n",
    "    embedded_sentences.append(emb_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4986 50\n"
     ]
    }
   ],
   "source": [
    "print(len(embedded_sentences),len(embedded_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF weighted word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TfidfVectorizer()\n",
    "model.fit(preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tf_weight = dict(zip(model.get_feature_names(),list(model.idf_))) #words as the key and the idf as value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4986/4986 [00:09<00:00, 519.54it/s]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences =[]\n",
    "for sentence in  tqdm(list_of_sentence):\n",
    "    emb_vec = np.zeros(50) #the words are 50 dim-vectors\n",
    "    sum_tfidf=0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in words_tf_weight:\n",
    "            vec= w2v_model.wv[word]\n",
    "            tf_idf = words_tf_weight[word]*(sentence.count(word)/len(sentence))\n",
    "            emb_vec += tf_idf*vec\n",
    "            sum_tfidf += tf_idf\n",
    "    \n",
    "    if (sum_tfidf!=0): emb_vec/=sum_tfidf\n",
    "    embedded_sentences.append(emb_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envai",
   "language": "python",
   "name": "envai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
